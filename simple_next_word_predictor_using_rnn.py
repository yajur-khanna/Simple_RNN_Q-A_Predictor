# -*- coding: utf-8 -*-
"""simple_next_word_predictor_using_RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aoy0B9nkrtFfqW9sagXl0Rgwz2zjschh
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn

df = pd.read_csv("/content/100_Unique_QA_Dataset.csv")
df.head()

# @title Tokenize
def tokenize(text):
  text = text.lower()
  text = text.replace("?", "")
  text = text.replace("'", "")
  return text.split()

tokenize("What is the capital of US?")

# @title Vocab

vocab = {'<UNK>': 0} # UNK represents unknown tokens, not in our dataset, set at index 0

def build_vocab(row):
  print(row['question'], row['answer'])
  tokenized_question = tokenize(row['question'])
  tokenized_answer = tokenize(row['answer'])

  merged_tokens = tokenized_question + tokenized_answer # Combined list of tokens in Q&A

  for token in merged_tokens:
    if token not in vocab:
      vocab[token] = len(vocab) # If there are 3 tokens in vocab (including UNK) then
      # current len = 3, the next token added will have value 3
      # vocab = {
          # UNK: 0
          # token1: 1
          # token2: 2 -> current len = 3
          # token3: 3
      # }

df.apply(build_vocab, axis = 1)

vocab

def text_to_indices(text, vocab):

  text_to_indices = []
  for text in tokenize(text):

    if text in vocab:
      text_to_indices.append(vocab[text])
    else:
      text_to_indices.append(vocab['<UNK>'])

  return text_to_indices

from torch.utils.data import Dataset, DataLoader

class QAdataset(Dataset):

  def __init__(self, df, vocab):
    self.df = df
    self.vocab = vocab

  def __len__(self):
    return self.df.shape[0]

  def __getitem__(self, idx):
    numerical_question = text_to_indices(self.df.iloc[idx]['question'], self.vocab)
    numerical_answer = text_to_indices(self.df.iloc[idx]['answer'], self.vocab)
    return torch.tensor(numerical_question), torch.tensor(numerical_answer)

dataset = QAdataset(df, vocab)
dataset[10]

dataloader = DataLoader(dataset, batch_size = 1, shuffle = True)

for question, answer in dataloader:
  print(question, answer)

for question, answer in dataloader:
  print(question, answer[0])

# @title RNN

dataset[0][0] # Question 1

x = nn.Embedding(324, embedding_dim=50)
c = x(dataset[0][0])
x(dataset[0][0]).shape # Each of the six words in the question are converted to a vector of size 50
# Total 6 vectors each of size 50

a = nn.RNN(50, 64)
y = a(c)
# y will be a tuple with 2 values
print(y[0].shape) # All o1 to o6, o1 is output for first word, then we send second word and o1
# to RNN layer to get o2, then we send third word and o2... till o6

# first value in the tuple is all values o1 through o6

# Second value in the tuple is the final output (i.e. o6) after all words have been sent
# to hidden layer
print(y[1].shape) # the second value in the tuple will be the last tensor in the first value of
# the tuple (which contains all outputs through time) in this case y[1] will be o6

# We can't use nn.Sequential because it expects one single value output from previous layer
# But y is a tuple with 2 values (all output values through time, final value)

print(f"Input tensor shape: {dataset[15][0]} = (8,) 1D tensor")
# Reshape will convert this to (1,8) 2D tensor, 1 is for batch

# Without reshape
a = nn.Embedding(324, embedding_dim=50)
b = a(dataset[15][0]) # .reshape converts tensor of size 8,1 to 1,8
print(f"Without reshaping input, Shape of b: {b.shape}")

# With reshape
a = nn.Embedding(324, embedding_dim=50)
b = a(dataset[15][0].reshape(1,8)) # .reshape converts tensor of size 8,1 to 1,8
print(f"With reshaping input, Shape of b: {b.shape}")

# RNN layer without batch_first = True (this keeps batch dimension first)
y = nn.RNN(50, 64)
d = y(b)
print(f"Without batch_first- Hidden output shape: {d[0].shape}, Final output shape: {d[1].shape}")

# RNN layer with batch_first = True (this keeps batch dimension first)
y = nn.RNN(50, 64, batch_first=True)
c = y(b)
print(f"With batch_first- Hidden output shape: {c[0].shape}, Final output shape: {c[1].shape}")
print("Final output: ", c[1])

# Output of final layer without batch_first
z1 = nn.Linear(64, 324)
e1 = z1(d[1])
print(f"Output of final layer without batch_first: {e1.shape}")

# Output of final layer with batch_first
z2 = nn.Linear(64, 324)
e2 = z2(c[1])
print(f"Output of final layer without batch_first: {e2.shape}")

# Even with batch_first we are getting 1, 8, 324 but we need a 1, 324 vector for loss calculation
e = e2.squeeze(0) # To remove the first 1
print(f"Final output logits shape after sequeezing: {e.shape}")

class SimpleRNN(nn.Module):

  def __init__(self, vocab_size):
    super().__init__()

    self.embedding = nn.Embedding(vocab_size, embedding_dim=50)
    self.RNN = nn.RNN(50, 64, batch_first=True) # Since we have 50 dimensional embeddings, input to RNN layer will be 50
    # We have 64 neurons in this layer

    self.fc = nn.Linear(64, vocab_size) # Output is vocab size as each neuron corresponds to
    # a word in the vocab, the answers are also part of vocab, neuron with highest value
    # output corresponds to predicted answer

  def forward(self, question):
    embedded_question = self.embedding(question)
    hidden, final = self.RNN(embedded_question)
    output = self.fc(final.squeeze(0))

    return output

lr = 0.001
epochs = 20
model = SimpleRNN(len(vocab))
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

for epoch in range(epochs):
  total_loss = 0
  for question, answer in dataloader:
    optimizer.zero_grad()
    out = model(question)
    loss = criterion(out, answer[0]) # Answer has shape 1, 1 we just want 1D tensor (single answer
    # to which logit with max probability will be used to calculate loss)
    loss.backward()
    optimizer.step()
    total_loss += loss.item()

  print(f"Epoch {epoch+1}: {total_loss:.4f}")

# @title Understanding dims=k

# dims = 1
probabilities1 = torch.tensor([
    [0.1, 0.3, 0.6],
    [0.8, 0.1, 0.1]
]) # (2, 3)

values_1, indices_1 = torch.max(probabilities1, dim = 1)
print(f"Output of values, indices across dimension 1: {values_1, indices_1}")

# dims = 2
probabilities2 = torch.tensor([
    [[0.1, 0.7, 0.2], [0.3, 0.4, 0.3]],
    [[0.8, 0.1, 0.1], [0.2, 0.2, 0.6]]
])  # (2, 2, 3)

values_2, indices_2 = torch.max(probabilities2, dim = 2)
print(f"Output of values, indices across dimension 1: {values_2, indices_2}")

# For questions the model hasn't seen
def predict(model, question, threshold=0.5):
  numerical_question = text_to_indices(question, vocab)
  question_tensor = torch.tensor(numerical_question).unsqueeze(0) # Add 1 to first dimension
  out = model(question_tensor) # Get logits

  # Convert logits to probs
  probs = torch.nn.functional.softmax(out, dim=1) # dim=1 => keep dim as 1

  value, index_of_max_prob = torch.max(probs, dim=1) # dim=1, take max along dimension 1

  if value < 0.5:
    print("I don't know")

  else:
    indices_in_vocab = list(vocab.keys())
    print(indices_in_vocab[index_of_max_prob])

predict(model, "What is the capital city of France?")

predict(model, "What do I do with I don't know?")

