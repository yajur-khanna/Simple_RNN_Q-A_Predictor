{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t7LT5etan5NR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/100_Unique_QA_Dataset.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Rip_1KptoDXu",
        "outputId": "2a7424a1-f983-4787-f3e2-e51a4975d889"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          question      answer\n",
              "0                   What is the capital of France?       Paris\n",
              "1                  What is the capital of Germany?      Berlin\n",
              "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
              "3  What is the largest planet in our solar system?     Jupiter\n",
              "4   What is the boiling point of water in Celsius?         100"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-968ba329-b4c8-4773-914e-959492d8ff02\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the capital of France?</td>\n",
              "      <td>Paris</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the capital of Germany?</td>\n",
              "      <td>Berlin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
              "      <td>Harper-Lee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the largest planet in our solar system?</td>\n",
              "      <td>Jupiter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the boiling point of water in Celsius?</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-968ba329-b4c8-4773-914e-959492d8ff02')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-968ba329-b4c8-4773-914e-959492d8ff02 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-968ba329-b4c8-4773-914e-959492d8ff02');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 90,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 90,\n        \"samples\": [\n          \"What is the currency of China?\",\n          \"What is the capital of Australia?\",\n          \"Who discovered electricity?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 85,\n        \"samples\": [\n          \"ChristopherColumbus\",\n          \"Paris\",\n          \"Christmas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tokenize\n",
        "def tokenize(text):\n",
        "  text = text.lower()\n",
        "  text = text.replace(\"?\", \"\")\n",
        "  text = text.replace(\"'\", \"\")\n",
        "  return text.split()"
      ],
      "metadata": {
        "id": "mJjxeXezoI5U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(\"What is the capital of US?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7chhaPxoW3c",
        "outputId": "739a6d58-7ba4-4320-8b0d-e84e48fd5718"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what', 'is', 'the', 'capital', 'of', 'us']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Vocab\n",
        "\n",
        "vocab = {'<UNK>': 0} # UNK represents unknown tokens, not in our dataset, set at index 0\n",
        "\n",
        "def build_vocab(row):\n",
        "  print(row['question'], row['answer'])\n",
        "  tokenized_question = tokenize(row['question'])\n",
        "  tokenized_answer = tokenize(row['answer'])\n",
        "\n",
        "  merged_tokens = tokenized_question + tokenized_answer # Combined list of tokens in Q&A\n",
        "\n",
        "  for token in merged_tokens:\n",
        "    if token not in vocab:\n",
        "      vocab[token] = len(vocab) # If there are 3 tokens in vocab (including UNK) then\n",
        "      # current len = 3, the next token added will have value 3\n",
        "      # vocab = {\n",
        "          # UNK: 0\n",
        "          # token1: 1\n",
        "          # token2: 2 -> current len = 3\n",
        "          # token3: 3\n",
        "      # }\n",
        "\n",
        "df.apply(build_vocab, axis = 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fhmMF4_dobE3",
        "outputId": "b524b649-9da2-4f61-d952-e3db7162eb9f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the capital of France? Paris\n",
            "What is the capital of Germany? Berlin\n",
            "Who wrote 'To Kill a Mockingbird'? Harper-Lee\n",
            "What is the largest planet in our solar system? Jupiter\n",
            "What is the boiling point of water in Celsius? 100\n",
            "Who painted the Mona Lisa? Leonardo-da-Vinci\n",
            "What is the square root of 64? 8\n",
            "What is the chemical symbol for gold? Au\n",
            "Which year did World War II end? 1945\n",
            "What is the longest river in the world? Nile\n",
            "What is the capital of Japan? Tokyo\n",
            "Who developed the theory of relativity? Albert-Einstein\n",
            "What is the freezing point of water in Fahrenheit? 32\n",
            "Which planet is known as the Red Planet? Mars\n",
            "Who is the author of '1984'? George-Orwell\n",
            "What is the currency of the United Kingdom? Pound\n",
            "What is the capital of India? Delhi\n",
            "Who discovered gravity? Newton\n",
            "How many continents are there on Earth? 7\n",
            "Which gas do plants use for photosynthesis? CO2\n",
            "What is the smallest prime number? 2\n",
            "Who invented the telephone? Alexander-Graham-Bell\n",
            "What is the capital of Australia? Canberra\n",
            "Which ocean is the largest? Pacific-Ocean\n",
            "What is the speed of light in vacuum? 299,792,458m/s\n",
            "Which language is spoken in Brazil? Portuguese\n",
            "Who discovered penicillin? Alexander-Fleming\n",
            "What is the capital of Canada? Ottawa\n",
            "What is the largest mammal on Earth? Whale\n",
            "Which element has the atomic number 1? Hydrogen\n",
            "What is the tallest mountain in the world? Everest\n",
            "Which city is known as the Big Apple? NewYork\n",
            "How many planets are in the Solar System? 8\n",
            "Who painted 'Starry Night'? vangogh\n",
            "What is the chemical formula of water? H2O\n",
            "What is the capital of Italy? Rome\n",
            "Which country is famous for sushi? Japan\n",
            "Who was the first person to step on the Moon? Armstrong\n",
            "What is the main ingredient in guacamole? Avocado\n",
            "How many sides does a hexagon have? 6\n",
            "What is the currency of China? Yuan\n",
            "Who wrote 'Pride and Prejudice'? Jane-Austen\n",
            "What is the chemical symbol for iron? Fe\n",
            "What is the hardest natural substance on Earth? Diamond\n",
            "Which continent is the largest by area? Asia\n",
            "Who was the first President of the United States? George-Washington\n",
            "Which bird is known for its ability to mimic sounds? Parrot\n",
            "What is the longest-running animated TV show? Simpsons\n",
            "What is the smallest country in the world? VaticanCity\n",
            "Which planet has the most moons? Saturn\n",
            "Who wrote 'Romeo and Juliet'? Shakespeare\n",
            "What is the main gas in Earth's atmosphere? Nitrogen\n",
            "How many bones are in the adult human body? 206\n",
            "Which metal is a liquid at room temperature? Mercury\n",
            "What is the capital of Russia? Moscow\n",
            "Who discovered electricity? Benjamin-Franklin\n",
            "Which is the second-largest country by land area? Canada\n",
            "What is the color of a ripe banana? Yellow\n",
            "Which month has 28 days in a common year? February\n",
            "What is the study of living organisms called? Biology\n",
            "Which country is home to the Great Wall? China\n",
            "What do bees collect from flowers? Nectar\n",
            "What is the opposite of 'day'? Night\n",
            "What is the capital of South Korea? Seoul\n",
            "Who invented the light bulb? Edison\n",
            "Which gas do humans breathe in for survival? Oxygen\n",
            "What is the square root of 144? 12\n",
            "Which country has the pyramids of Giza? Egypt\n",
            "Which sea creature has eight arms? Octopus\n",
            "Which holiday is celebrated on December 25? Christmas\n",
            "What is the currency of Japan? Yen\n",
            "How many legs does a spider have? 8\n",
            "Which sport uses a net, ball, and hoop? Basketball\n",
            "Which country is famous for its kangaroos? Australia\n",
            "Who was the first female Prime Minister of the UK? MargaretThatcher\n",
            "Which is the fastest land animal? Cheetah\n",
            "What is the first element on the periodic table? Hydrogen\n",
            "What is the capital of Spain? Madrid\n",
            "Which planet is the closest to the Sun? Mercury\n",
            "Who is known as the father of computers? CharlesBabbage\n",
            "What is the capital of Mexico? MexicoCity\n",
            "How many colors are in a rainbow? 7\n",
            "Which musical instrument has black and white keys? Piano\n",
            "Who discovered the Americas in 1492? ChristopherColumbus\n",
            "Which Disney character has a long nose and grows it when lying? Pinocchio\n",
            "Who directed the movie 'Titanic'? JamesCameron\n",
            "Which superhero is also known as the Dark Knight? Batman\n",
            "What is the capital of Brazil? Brasilia\n",
            "Which fruit is known as the king of fruits? Mango\n",
            "Which country is known for the Eiffel Tower? France\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     None\n",
              "1     None\n",
              "2     None\n",
              "3     None\n",
              "4     None\n",
              "      ... \n",
              "85    None\n",
              "86    None\n",
              "87    None\n",
              "88    None\n",
              "89    None\n",
              "Length: 90, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nNwIriZpWTi",
        "outputId": "83c53ba4-f40f-4f1f-aae5-b7214a059ff9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<UNK>': 0,\n",
              " 'what': 1,\n",
              " 'is': 2,\n",
              " 'the': 3,\n",
              " 'capital': 4,\n",
              " 'of': 5,\n",
              " 'france': 6,\n",
              " 'paris': 7,\n",
              " 'germany': 8,\n",
              " 'berlin': 9,\n",
              " 'who': 10,\n",
              " 'wrote': 11,\n",
              " 'to': 12,\n",
              " 'kill': 13,\n",
              " 'a': 14,\n",
              " 'mockingbird': 15,\n",
              " 'harper-lee': 16,\n",
              " 'largest': 17,\n",
              " 'planet': 18,\n",
              " 'in': 19,\n",
              " 'our': 20,\n",
              " 'solar': 21,\n",
              " 'system': 22,\n",
              " 'jupiter': 23,\n",
              " 'boiling': 24,\n",
              " 'point': 25,\n",
              " 'water': 26,\n",
              " 'celsius': 27,\n",
              " '100': 28,\n",
              " 'painted': 29,\n",
              " 'mona': 30,\n",
              " 'lisa': 31,\n",
              " 'leonardo-da-vinci': 32,\n",
              " 'square': 33,\n",
              " 'root': 34,\n",
              " '64': 35,\n",
              " '8': 36,\n",
              " 'chemical': 37,\n",
              " 'symbol': 38,\n",
              " 'for': 39,\n",
              " 'gold': 40,\n",
              " 'au': 41,\n",
              " 'which': 42,\n",
              " 'year': 43,\n",
              " 'did': 44,\n",
              " 'world': 45,\n",
              " 'war': 46,\n",
              " 'ii': 47,\n",
              " 'end': 48,\n",
              " '1945': 49,\n",
              " 'longest': 50,\n",
              " 'river': 51,\n",
              " 'nile': 52,\n",
              " 'japan': 53,\n",
              " 'tokyo': 54,\n",
              " 'developed': 55,\n",
              " 'theory': 56,\n",
              " 'relativity': 57,\n",
              " 'albert-einstein': 58,\n",
              " 'freezing': 59,\n",
              " 'fahrenheit': 60,\n",
              " '32': 61,\n",
              " 'known': 62,\n",
              " 'as': 63,\n",
              " 'red': 64,\n",
              " 'mars': 65,\n",
              " 'author': 66,\n",
              " '1984': 67,\n",
              " 'george-orwell': 68,\n",
              " 'currency': 69,\n",
              " 'united': 70,\n",
              " 'kingdom': 71,\n",
              " 'pound': 72,\n",
              " 'india': 73,\n",
              " 'delhi': 74,\n",
              " 'discovered': 75,\n",
              " 'gravity': 76,\n",
              " 'newton': 77,\n",
              " 'how': 78,\n",
              " 'many': 79,\n",
              " 'continents': 80,\n",
              " 'are': 81,\n",
              " 'there': 82,\n",
              " 'on': 83,\n",
              " 'earth': 84,\n",
              " '7': 85,\n",
              " 'gas': 86,\n",
              " 'do': 87,\n",
              " 'plants': 88,\n",
              " 'use': 89,\n",
              " 'photosynthesis': 90,\n",
              " 'co2': 91,\n",
              " 'smallest': 92,\n",
              " 'prime': 93,\n",
              " 'number': 94,\n",
              " '2': 95,\n",
              " 'invented': 96,\n",
              " 'telephone': 97,\n",
              " 'alexander-graham-bell': 98,\n",
              " 'australia': 99,\n",
              " 'canberra': 100,\n",
              " 'ocean': 101,\n",
              " 'pacific-ocean': 102,\n",
              " 'speed': 103,\n",
              " 'light': 104,\n",
              " 'vacuum': 105,\n",
              " '299,792,458m/s': 106,\n",
              " 'language': 107,\n",
              " 'spoken': 108,\n",
              " 'brazil': 109,\n",
              " 'portuguese': 110,\n",
              " 'penicillin': 111,\n",
              " 'alexander-fleming': 112,\n",
              " 'canada': 113,\n",
              " 'ottawa': 114,\n",
              " 'mammal': 115,\n",
              " 'whale': 116,\n",
              " 'element': 117,\n",
              " 'has': 118,\n",
              " 'atomic': 119,\n",
              " '1': 120,\n",
              " 'hydrogen': 121,\n",
              " 'tallest': 122,\n",
              " 'mountain': 123,\n",
              " 'everest': 124,\n",
              " 'city': 125,\n",
              " 'big': 126,\n",
              " 'apple': 127,\n",
              " 'newyork': 128,\n",
              " 'planets': 129,\n",
              " 'starry': 130,\n",
              " 'night': 131,\n",
              " 'vangogh': 132,\n",
              " 'formula': 133,\n",
              " 'h2o': 134,\n",
              " 'italy': 135,\n",
              " 'rome': 136,\n",
              " 'country': 137,\n",
              " 'famous': 138,\n",
              " 'sushi': 139,\n",
              " 'was': 140,\n",
              " 'first': 141,\n",
              " 'person': 142,\n",
              " 'step': 143,\n",
              " 'moon': 144,\n",
              " 'armstrong': 145,\n",
              " 'main': 146,\n",
              " 'ingredient': 147,\n",
              " 'guacamole': 148,\n",
              " 'avocado': 149,\n",
              " 'sides': 150,\n",
              " 'does': 151,\n",
              " 'hexagon': 152,\n",
              " 'have': 153,\n",
              " '6': 154,\n",
              " 'china': 155,\n",
              " 'yuan': 156,\n",
              " 'pride': 157,\n",
              " 'and': 158,\n",
              " 'prejudice': 159,\n",
              " 'jane-austen': 160,\n",
              " 'iron': 161,\n",
              " 'fe': 162,\n",
              " 'hardest': 163,\n",
              " 'natural': 164,\n",
              " 'substance': 165,\n",
              " 'diamond': 166,\n",
              " 'continent': 167,\n",
              " 'by': 168,\n",
              " 'area': 169,\n",
              " 'asia': 170,\n",
              " 'president': 171,\n",
              " 'states': 172,\n",
              " 'george-washington': 173,\n",
              " 'bird': 174,\n",
              " 'its': 175,\n",
              " 'ability': 176,\n",
              " 'mimic': 177,\n",
              " 'sounds': 178,\n",
              " 'parrot': 179,\n",
              " 'longest-running': 180,\n",
              " 'animated': 181,\n",
              " 'tv': 182,\n",
              " 'show': 183,\n",
              " 'simpsons': 184,\n",
              " 'vaticancity': 185,\n",
              " 'most': 186,\n",
              " 'moons': 187,\n",
              " 'saturn': 188,\n",
              " 'romeo': 189,\n",
              " 'juliet': 190,\n",
              " 'shakespeare': 191,\n",
              " 'earths': 192,\n",
              " 'atmosphere': 193,\n",
              " 'nitrogen': 194,\n",
              " 'bones': 195,\n",
              " 'adult': 196,\n",
              " 'human': 197,\n",
              " 'body': 198,\n",
              " '206': 199,\n",
              " 'metal': 200,\n",
              " 'liquid': 201,\n",
              " 'at': 202,\n",
              " 'room': 203,\n",
              " 'temperature': 204,\n",
              " 'mercury': 205,\n",
              " 'russia': 206,\n",
              " 'moscow': 207,\n",
              " 'electricity': 208,\n",
              " 'benjamin-franklin': 209,\n",
              " 'second-largest': 210,\n",
              " 'land': 211,\n",
              " 'color': 212,\n",
              " 'ripe': 213,\n",
              " 'banana': 214,\n",
              " 'yellow': 215,\n",
              " 'month': 216,\n",
              " '28': 217,\n",
              " 'days': 218,\n",
              " 'common': 219,\n",
              " 'february': 220,\n",
              " 'study': 221,\n",
              " 'living': 222,\n",
              " 'organisms': 223,\n",
              " 'called': 224,\n",
              " 'biology': 225,\n",
              " 'home': 226,\n",
              " 'great': 227,\n",
              " 'wall': 228,\n",
              " 'bees': 229,\n",
              " 'collect': 230,\n",
              " 'from': 231,\n",
              " 'flowers': 232,\n",
              " 'nectar': 233,\n",
              " 'opposite': 234,\n",
              " 'day': 235,\n",
              " 'south': 236,\n",
              " 'korea': 237,\n",
              " 'seoul': 238,\n",
              " 'bulb': 239,\n",
              " 'edison': 240,\n",
              " 'humans': 241,\n",
              " 'breathe': 242,\n",
              " 'survival': 243,\n",
              " 'oxygen': 244,\n",
              " '144': 245,\n",
              " '12': 246,\n",
              " 'pyramids': 247,\n",
              " 'giza': 248,\n",
              " 'egypt': 249,\n",
              " 'sea': 250,\n",
              " 'creature': 251,\n",
              " 'eight': 252,\n",
              " 'arms': 253,\n",
              " 'octopus': 254,\n",
              " 'holiday': 255,\n",
              " 'celebrated': 256,\n",
              " 'december': 257,\n",
              " '25': 258,\n",
              " 'christmas': 259,\n",
              " 'yen': 260,\n",
              " 'legs': 261,\n",
              " 'spider': 262,\n",
              " 'sport': 263,\n",
              " 'uses': 264,\n",
              " 'net,': 265,\n",
              " 'ball,': 266,\n",
              " 'hoop': 267,\n",
              " 'basketball': 268,\n",
              " 'kangaroos': 269,\n",
              " 'female': 270,\n",
              " 'minister': 271,\n",
              " 'uk': 272,\n",
              " 'margaretthatcher': 273,\n",
              " 'fastest': 274,\n",
              " 'animal': 275,\n",
              " 'cheetah': 276,\n",
              " 'periodic': 277,\n",
              " 'table': 278,\n",
              " 'spain': 279,\n",
              " 'madrid': 280,\n",
              " 'closest': 281,\n",
              " 'sun': 282,\n",
              " 'father': 283,\n",
              " 'computers': 284,\n",
              " 'charlesbabbage': 285,\n",
              " 'mexico': 286,\n",
              " 'mexicocity': 287,\n",
              " 'colors': 288,\n",
              " 'rainbow': 289,\n",
              " 'musical': 290,\n",
              " 'instrument': 291,\n",
              " 'black': 292,\n",
              " 'white': 293,\n",
              " 'keys': 294,\n",
              " 'piano': 295,\n",
              " 'americas': 296,\n",
              " '1492': 297,\n",
              " 'christophercolumbus': 298,\n",
              " 'disney': 299,\n",
              " 'character': 300,\n",
              " 'long': 301,\n",
              " 'nose': 302,\n",
              " 'grows': 303,\n",
              " 'it': 304,\n",
              " 'when': 305,\n",
              " 'lying': 306,\n",
              " 'pinocchio': 307,\n",
              " 'directed': 308,\n",
              " 'movie': 309,\n",
              " 'titanic': 310,\n",
              " 'jamescameron': 311,\n",
              " 'superhero': 312,\n",
              " 'also': 313,\n",
              " 'dark': 314,\n",
              " 'knight': 315,\n",
              " 'batman': 316,\n",
              " 'brasilia': 317,\n",
              " 'fruit': 318,\n",
              " 'king': 319,\n",
              " 'fruits': 320,\n",
              " 'mango': 321,\n",
              " 'eiffel': 322,\n",
              " 'tower': 323}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(text, vocab):\n",
        "\n",
        "  text_to_indices = []\n",
        "  for text in tokenize(text):\n",
        "\n",
        "    if text in vocab:\n",
        "      text_to_indices.append(vocab[text])\n",
        "    else:\n",
        "      text_to_indices.append(vocab['<UNK>'])\n",
        "\n",
        "  return text_to_indices"
      ],
      "metadata": {
        "id": "Z5x_Qd_urVab"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "8QFXNeSqumLU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QAdataset(Dataset):\n",
        "\n",
        "  def __init__(self, df, vocab):\n",
        "    self.df = df\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.df.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    numerical_question = text_to_indices(self.df.iloc[idx]['question'], self.vocab)\n",
        "    numerical_answer = text_to_indices(self.df.iloc[idx]['answer'], self.vocab)\n",
        "    return torch.tensor(numerical_question), torch.tensor(numerical_answer)"
      ],
      "metadata": {
        "id": "jbkXzjFjupe-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = QAdataset(df, vocab)\n",
        "dataset[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLiLcsH7vzJF",
        "outputId": "96284638-1190-4e66-8477-3cdfbf4d4c71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 1,  2,  3,  4,  5, 53]), tensor([54]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size = 1, shuffle = True)"
      ],
      "metadata": {
        "id": "7OqJux0Dv42J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question, answer in dataloader:\n",
        "  print(question, answer)\n",
        "\n",
        "for question, answer in dataloader:\n",
        "  print(question, answer[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC0B9yLwwc6E",
        "outputId": "8ad6bc2b-5b04-4d64-f1cd-b4a0184d571d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([[23]])\n",
            "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([[28]])\n",
            "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([[61]])\n",
            "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([[287]])\n",
            "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([[52]])\n",
            "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([[155]])\n",
            "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([[295]])\n",
            "tensor([[ 10,  75, 111]]) tensor([[112]])\n",
            "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([[53]])\n",
            "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([[185]])\n",
            "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([[65]])\n",
            "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([[298]])\n",
            "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([[6]])\n",
            "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([[106]])\n",
            "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([[179]])\n",
            "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([[166]])\n",
            "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([[156]])\n",
            "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([[100]])\n",
            "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([[114]])\n",
            "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([[131]])\n",
            "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([[41]])\n",
            "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([[280]])\n",
            "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([[173]])\n",
            "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([[74]])\n",
            "tensor([[ 10, 308,   3, 309, 310]]) tensor([[311]])\n",
            "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([[128]])\n",
            "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([[220]])\n",
            "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([[285]])\n",
            "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([[72]])\n",
            "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([[238]])\n",
            "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([[145]])\n",
            "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([[321]])\n",
            "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([[194]])\n",
            "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([[149]])\n",
            "tensor([[10, 11, 12, 13, 14, 15]]) tensor([[16]])\n",
            "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([[95]])\n",
            "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([[124]])\n",
            "tensor([[10, 29,  3, 30, 31]]) tensor([[32]])\n",
            "tensor([[ 42, 101,   2,   3,  17]]) tensor([[102]])\n",
            "tensor([[10, 55,  3, 56,  5, 57]]) tensor([[58]])\n",
            "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([[259]])\n",
            "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([[91]])\n",
            "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([[116]])\n",
            "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([[54]])\n",
            "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([[273]])\n",
            "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([[233]])\n",
            "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([[317]])\n",
            "tensor([[ 10,  29, 130, 131]]) tensor([[132]])\n",
            "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([[99]])\n",
            "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([[36]])\n",
            "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([[249]])\n",
            "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([[244]])\n",
            "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([[162]])\n",
            "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([[254]])\n",
            "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([[134]])\n",
            "tensor([[ 10,  75, 208]]) tensor([[209]])\n",
            "tensor([[ 10,  96,   3, 104, 239]]) tensor([[240]])\n",
            "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([[136]])\n",
            "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([[316]])\n",
            "tensor([[ 10,  11, 157, 158, 159]]) tensor([[160]])\n",
            "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([[121]])\n",
            "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([[49]])\n",
            "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([[260]])\n",
            "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([[207]])\n",
            "tensor([[10,  2,  3, 66,  5, 67]]) tensor([[68]])\n",
            "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([[113]])\n",
            "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([[215]])\n",
            "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([[36]])\n",
            "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([[170]])\n",
            "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([[184]])\n",
            "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([[188]])\n",
            "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([[276]])\n",
            "tensor([[ 10,  11, 189, 158, 190]]) tensor([[191]])\n",
            "tensor([[10, 75, 76]]) tensor([[77]])\n",
            "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([[205]])\n",
            "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([[154]])\n",
            "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([[85]])\n",
            "tensor([[10, 96,  3, 97]]) tensor([[98]])\n",
            "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([[268]])\n",
            "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([[307]])\n",
            "tensor([[1, 2, 3, 4, 5, 8]]) tensor([[9]])\n",
            "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([[36]])\n",
            "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([[121]])\n",
            "tensor([[1, 2, 3, 4, 5, 6]]) tensor([[7]])\n",
            "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([[225]])\n",
            "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([[246]])\n",
            "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([[199]])\n",
            "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([[85]])\n",
            "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([[205]])\n",
            "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([[110]])\n",
            "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([220])\n",
            "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([207])\n",
            "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([155])\n",
            "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([166])\n",
            "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([100])\n",
            "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([106])\n",
            "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([188])\n",
            "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([194])\n",
            "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([317])\n",
            "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([238])\n",
            "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([23])\n",
            "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([128])\n",
            "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([72])\n",
            "tensor([[ 42, 101,   2,   3,  17]]) tensor([102])\n",
            "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([287])\n",
            "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([260])\n",
            "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([36])\n",
            "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([124])\n",
            "tensor([[ 10,  96,   3, 104, 239]]) tensor([240])\n",
            "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([295])\n",
            "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([36])\n",
            "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([276])\n",
            "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([259])\n",
            "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([225])\n",
            "tensor([[1, 2, 3, 4, 5, 8]]) tensor([9])\n",
            "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([52])\n",
            "tensor([[ 10,  75, 208]]) tensor([209])\n",
            "tensor([[ 10,  11, 189, 158, 190]]) tensor([191])\n",
            "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([280])\n",
            "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([131])\n",
            "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([233])\n",
            "tensor([[1, 2, 3, 4, 5, 6]]) tensor([7])\n",
            "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([53])\n",
            "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([268])\n",
            "tensor([[ 10, 308,   3, 309, 310]]) tensor([311])\n",
            "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([285])\n",
            "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([28])\n",
            "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([99])\n",
            "tensor([[ 10,  75, 111]]) tensor([112])\n",
            "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([254])\n",
            "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([307])\n",
            "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([91])\n",
            "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([298])\n",
            "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([145])\n",
            "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([249])\n",
            "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([173])\n",
            "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([170])\n",
            "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([65])\n",
            "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([273])\n",
            "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([61])\n",
            "tensor([[10, 75, 76]]) tensor([77])\n",
            "tensor([[10, 55,  3, 56,  5, 57]]) tensor([58])\n",
            "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([74])\n",
            "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([184])\n",
            "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([185])\n",
            "tensor([[10,  2,  3, 66,  5, 67]]) tensor([68])\n",
            "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([36])\n",
            "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([199])\n",
            "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([113])\n",
            "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([162])\n",
            "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([95])\n",
            "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([114])\n",
            "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([316])\n",
            "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([6])\n",
            "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([85])\n",
            "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([149])\n",
            "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([154])\n",
            "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([205])\n",
            "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([121])\n",
            "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([54])\n",
            "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([215])\n",
            "tensor([[10, 11, 12, 13, 14, 15]]) tensor([16])\n",
            "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([134])\n",
            "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([321])\n",
            "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([121])\n",
            "tensor([[ 10,  11, 157, 158, 159]]) tensor([160])\n",
            "tensor([[ 10,  29, 130, 131]]) tensor([132])\n",
            "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([136])\n",
            "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([49])\n",
            "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([179])\n",
            "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([116])\n",
            "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([205])\n",
            "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([85])\n",
            "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([41])\n",
            "tensor([[10, 29,  3, 30, 31]]) tensor([32])\n",
            "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([246])\n",
            "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([244])\n",
            "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([156])\n",
            "tensor([[10, 96,  3, 97]]) tensor([98])\n",
            "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([110])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RNN\n",
        "\n",
        "dataset[0][0] # Question 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTDeQGyUz9Vu",
        "outputId": "a8bfebad-e6ba-41d2-9cda-65f244c9d238"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = nn.Embedding(324, embedding_dim=50)\n",
        "c = x(dataset[0][0])\n",
        "x(dataset[0][0]).shape # Each of the six words in the question are converted to a vector of size 50\n",
        "# Total 6 vectors each of size 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycbPvWh7zvIp",
        "outputId": "e708a8e1-2b36-4915-86ae-033d339a0572"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = nn.RNN(50, 64)\n",
        "y = a(c)\n",
        "# y will be a tuple with 2 values\n",
        "print(y[0].shape) # All o1 to o6, o1 is output for first word, then we send second word and o1\n",
        "# to RNN layer to get o2, then we send third word and o2... till o6\n",
        "\n",
        "# first value in the tuple is all values o1 through o6\n",
        "\n",
        "# Second value in the tuple is the final output (i.e. o6) after all words have been sent\n",
        "# to hidden layer\n",
        "print(y[1].shape) # the second value in the tuple will be the last tensor in the first value of\n",
        "# the tuple (which contains all outputs through time) in this case y[1] will be o6\n",
        "\n",
        "# We can't use nn.Sequential because it expects one single value output from previous layer\n",
        "# But y is a tuple with 2 values (all output values through time, final value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbX8OaXF0I_7",
        "outputId": "b1e90dae-2946-4b6f-8bde-69a2e5efdc98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 64])\n",
            "torch.Size([1, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input tensor shape: {dataset[15][0]} = (8,) 1D tensor\")\n",
        "# Reshape will convert this to (1,8) 2D tensor, 1 is for batch\n",
        "\n",
        "# Without reshape\n",
        "a = nn.Embedding(324, embedding_dim=50)\n",
        "b = a(dataset[15][0]) # .reshape converts tensor of size 8,1 to 1,8\n",
        "print(f\"Without reshaping input, Shape of b: {b.shape}\")\n",
        "\n",
        "# With reshape\n",
        "a = nn.Embedding(324, embedding_dim=50)\n",
        "b = a(dataset[15][0].reshape(1,8)) # .reshape converts tensor of size 8,1 to 1,8\n",
        "print(f\"With reshaping input, Shape of b: {b.shape}\")\n",
        "\n",
        "# RNN layer without batch_first = True (this keeps batch dimension first)\n",
        "y = nn.RNN(50, 64)\n",
        "d = y(b)\n",
        "print(f\"Without batch_first- Hidden output shape: {d[0].shape}, Final output shape: {d[1].shape}\")\n",
        "\n",
        "# RNN layer with batch_first = True (this keeps batch dimension first)\n",
        "y = nn.RNN(50, 64, batch_first=True)\n",
        "c = y(b)\n",
        "print(f\"With batch_first- Hidden output shape: {c[0].shape}, Final output shape: {c[1].shape}\")\n",
        "print(\"Final output: \", c[1])\n",
        "\n",
        "# Output of final layer without batch_first\n",
        "z1 = nn.Linear(64, 324)\n",
        "e1 = z1(d[1])\n",
        "print(f\"Output of final layer without batch_first: {e1.shape}\")\n",
        "\n",
        "# Output of final layer with batch_first\n",
        "z2 = nn.Linear(64, 324)\n",
        "e2 = z2(c[1])\n",
        "print(f\"Output of final layer without batch_first: {e2.shape}\")\n",
        "\n",
        "# Even with batch_first we are getting 1, 8, 324 but we need a 1, 324 vector for loss calculation\n",
        "e = e2.squeeze(0) # To remove the first 1\n",
        "print(f\"Final output logits shape after sequeezing: {e.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlgubXJjEtSe",
        "outputId": "87fff93f-2dcb-4df5-bd0d-017d6e5ec457"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: tensor([ 1,  2,  3, 69,  5,  3, 70, 71]) = (8,) 1D tensor\n",
            "Without reshaping input, Shape of b: torch.Size([8, 50])\n",
            "With reshaping input, Shape of b: torch.Size([1, 8, 50])\n",
            "Without batch_first- Hidden output shape: torch.Size([1, 8, 64]), Final output shape: torch.Size([1, 8, 64])\n",
            "With batch_first- Hidden output shape: torch.Size([1, 8, 64]), Final output shape: torch.Size([1, 1, 64])\n",
            "Final output:  tensor([[[-0.6299,  0.5277,  0.7710,  0.4048,  0.7291, -0.3848, -0.5765,\n",
            "           0.3769,  0.1551,  0.4197, -0.3704,  0.1178,  0.4352, -0.1122,\n",
            "          -0.1843, -0.5328,  0.0880, -0.3082, -0.0777, -0.0728,  0.1416,\n",
            "           0.6438,  0.8491,  0.5066,  0.5593,  0.4331,  0.1472, -0.3177,\n",
            "          -0.5275, -0.9150, -0.4234,  0.8432, -0.4150,  0.1030,  0.6830,\n",
            "          -0.2814,  0.7304, -0.2990,  0.1383,  0.3723,  0.0532, -0.0361,\n",
            "          -0.5078,  0.3440,  0.2258, -0.3793,  0.4569, -0.5576, -0.6436,\n",
            "          -0.2556, -0.5936,  0.2188, -0.8297, -0.6415,  0.1542, -0.2262,\n",
            "           0.7372,  0.4538,  0.4253,  0.2391, -0.4250,  0.0990, -0.1882,\n",
            "           0.4259]]], grad_fn=<StackBackward0>)\n",
            "Output of final layer without batch_first: torch.Size([1, 8, 324])\n",
            "Output of final layer without batch_first: torch.Size([1, 1, 324])\n",
            "Final output logits shape after sequeezing: torch.Size([1, 324])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim=50)\n",
        "    self.RNN = nn.RNN(50, 64, batch_first=True) # Since we have 50 dimensional embeddings, input to RNN layer will be 50\n",
        "    # We have 64 neurons in this layer\n",
        "\n",
        "    self.fc = nn.Linear(64, vocab_size) # Output is vocab size as each neuron corresponds to\n",
        "    # a word in the vocab, the answers are also part of vocab, neuron with highest value\n",
        "    # output corresponds to predicted answer\n",
        "\n",
        "  def forward(self, question):\n",
        "    embedded_question = self.embedding(question)\n",
        "    hidden, final = self.RNN(embedded_question)\n",
        "    output = self.fc(final.squeeze(0))\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "4ya7OzpUwmZn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "epochs = 20\n",
        "model = SimpleRNN(len(vocab))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "0xpaicQI2tqR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "  for question, answer in dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    out = model(question)\n",
        "    loss = criterion(out, answer[0]) # Answer has shape 1, 1 we just want 1D tensor (single answer\n",
        "    # to which logit with max probability will be used to calculate loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch+1}: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rgW0IpW6l9F",
        "outputId": "aaffaa17-d1ed-4632-ba47-7c3c6e068566"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: 527.7548\n",
            "Epoch 2: 455.5189\n",
            "Epoch 3: 376.1973\n",
            "Epoch 4: 317.6182\n",
            "Epoch 5: 267.2670\n",
            "Epoch 6: 219.8879\n",
            "Epoch 7: 173.9641\n",
            "Epoch 8: 135.1649\n",
            "Epoch 9: 102.9909\n",
            "Epoch 10: 78.1373\n",
            "Epoch 11: 59.0848\n",
            "Epoch 12: 46.1155\n",
            "Epoch 13: 36.2798\n",
            "Epoch 14: 29.2895\n",
            "Epoch 15: 24.0680\n",
            "Epoch 16: 19.7963\n",
            "Epoch 17: 16.9372\n",
            "Epoch 18: 14.4775\n",
            "Epoch 19: 12.4468\n",
            "Epoch 20: 10.8186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Understanding dims=k\n",
        "\n",
        "# dims = 1\n",
        "probabilities1 = torch.tensor([\n",
        "    [0.1, 0.3, 0.6],\n",
        "    [0.8, 0.1, 0.1]\n",
        "]) # (2, 3)\n",
        "\n",
        "values_1, indices_1 = torch.max(probabilities1, dim = 1)\n",
        "print(f\"Output of values, indices across dimension 1: {values_1, indices_1}\")\n",
        "\n",
        "# dims = 2\n",
        "probabilities2 = torch.tensor([\n",
        "    [[0.1, 0.7, 0.2], [0.3, 0.4, 0.3]],\n",
        "    [[0.8, 0.1, 0.1], [0.2, 0.2, 0.6]]\n",
        "])  # (2, 2, 3)\n",
        "\n",
        "values_2, indices_2 = torch.max(probabilities2, dim = 2)\n",
        "print(f\"Output of values, indices across dimension 1: {values_2, indices_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCd5vKo7YF5y",
        "outputId": "6a318fcc-6e6a-4987-a223-1ecad2213a26"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of values, indices across dimension 1: (tensor([0.6000, 0.8000]), tensor([2, 0]))\n",
            "Output of values, indices across dimension 1: (tensor([[0.7000, 0.4000],\n",
            "        [0.8000, 0.6000]]), tensor([[1, 1],\n",
            "        [0, 2]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For questions the model hasn't seen\n",
        "def predict(model, question, threshold=0.5):\n",
        "  numerical_question = text_to_indices(question, vocab)\n",
        "  question_tensor = torch.tensor(numerical_question).unsqueeze(0) # Add 1 to first dimension\n",
        "  out = model(question_tensor) # Get logits\n",
        "\n",
        "  # Convert logits to probs\n",
        "  probs = torch.nn.functional.softmax(out, dim=1) # dim=1 => keep dim as 1\n",
        "\n",
        "  value, index_of_max_prob = torch.max(probs, dim=1) # dim=1, take max along dimension 1\n",
        "\n",
        "  if value < 0.5:\n",
        "    print(\"I don't know\")\n",
        "\n",
        "  else:\n",
        "    indices_in_vocab = list(vocab.keys())\n",
        "    print(indices_in_vocab[index_of_max_prob])"
      ],
      "metadata": {
        "id": "EZMuISHo7kyz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, \"What is the capital city of France?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3anAS4TraNjL",
        "outputId": "f6287e60-6acb-4929-e895-33db98d228fa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, \"What do I do with I don't know?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0TkQ1FcaaSt",
        "outputId": "7a6ba5ef-9d24-4229-fae1-a11746896f24"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_7zqHk_za-Fo"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}